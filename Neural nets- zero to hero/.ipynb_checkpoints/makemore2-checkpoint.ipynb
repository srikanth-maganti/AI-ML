{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d5c11a8-670b-4d0b-b1ca-320ca33f49a2",
   "metadata": {},
   "source": [
    "## Character level language model which considers multiple previous characters using Multilayer Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573210f6-afb6-4faf-8629-af5d0cf9a65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#points from research paper\n",
    "#it is word level language model\n",
    "#predicts probability of next word based on previous three words\n",
    "#every input word embedded into some 30 0r 60  dimensional feature vector\n",
    "#which means there are 17000 words in 30 or 60 dimensional feature space\n",
    "#words with similar meaning or context are close to each other ,Generality\n",
    "#three layers\n",
    "#1.input layer takes index of words and check lookup table for embedding\n",
    "#2hidden layer with tanh activation function\n",
    "#3.softmax layer with 170000 classes fully connected to hidden layer(more computation here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4b1480-374a-46fb-87d1-bbe2c6ff473e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277b0411-90c3-4f7f-9ee0-826e729900f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=open(\"names.txt\",\"r\").read().splitlines()\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e090f66c-eaf2-47d5-a250-8dadfa9dd35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars=sorted(list(set(''.join(words))))\n",
    "stoi={ch:i for i,ch in enumerate(['.']+chars)}\n",
    "\n",
    "itos={i:ch for ch,i in stoi.items()}\n",
    "itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c98435-44cb-482e-85fd-2b24fa03eedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "Y=[]\n",
    "block_size=3\n",
    "\n",
    "for word in words[:5]:\n",
    "    context=[0]*block_size\n",
    "    for ch in word+'.':\n",
    "        X.append(context)\n",
    "     \n",
    "        ix=stoi[ch]\n",
    "        Y.append(ix)\n",
    "        print(''.join([itos[i] for i in context]),\"--->\",ch)\n",
    "        context=context[1:]+[ix]\n",
    "        \n",
    "X=torch.tensor(X)\n",
    "Y=torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cec3f92-3b74-43de-96c7-c780e0286da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape,X.dtype,Y.shape,Y.dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcb142e-b28a-4689-a12f-3e7c259dcdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "C=torch.randn((27,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe55ead-01f7-4fd4-8864-1e8429538d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb=C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383c3fa0-d01c-405c-b5e2-8d0f52535535",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1=torch.randn((6,100))\n",
    "b1=torch.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb1a08b-186f-498b-80f2-228411482aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([emb[:,0,:],emb[:,1,:],emb[:,2,:]],dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49b85f6-4f69-4234-afce-b6de5af72fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat(torch.unbind(emb,1),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76881cf3-6c15-4690-877b-8363d4e6bca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.arange(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfc7cc2-aab6-4738-88db-b54e0f0a2882",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.view(9,2) # extreamly efffient than previous because it doesnt involves storage manupulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b823202-072b-4262-8b59-df4bda858029",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167754d3-ac90-47e3-9552-e9330f65b503",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb.view(32,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3976515-201e-4082-9661-4441488a84bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "h=torch.tanh(emb.view(-1,6)@W1+b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9252fe18-1d6f-4204-ae3d-5b4c8ebb79a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9771e5f9-aa74-45b4-a194-8e2fd329f64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nxt layer\n",
    "W2=torch.randn((100,27))\n",
    "b2=torch.randn(27)\n",
    "logits=h@W2+b2\n",
    "# print(logits.shape)\n",
    "counts=logits.exp()\n",
    "probs=counts/counts.sum(1,keepdim=True)\n",
    "# probs[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4befc5ac-ba36-48a0-a31f-7d4d56945cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss=-probs[torch.arange(32),Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d188ad97-176d-487b-a3d4-552c838be0d0",
   "metadata": {},
   "source": [
    "## Now made respectable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648a9e06-caef-4ef9-8810-3e579f94d08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build dataset\n",
    "def build_dataset(words):\n",
    "        X=[]\n",
    "        Y=[]\n",
    "        block_size=3\n",
    "        \n",
    "        for word in words:\n",
    "            context=[0]*block_size\n",
    "            for ch in word+'.':\n",
    "                X.append(context)\n",
    "             \n",
    "                ix=stoi[ch]\n",
    "                Y.append(ix)\n",
    "                \n",
    "                context=context[1:]+[ix]\n",
    "                \n",
    "        X=torch.tensor(X)\n",
    "        Y=torch.tensor(Y)\n",
    "        print(X.shape,Y.shape)\n",
    "        return X,Y\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1=int(0.8*len(words))\n",
    "n2=int(0.9*len(words))\n",
    "Xtre,Ytre=build_dataset(words[:n1])\n",
    "Xval,Yval=build_dataset(words[n1:n2])\n",
    "Xte,Yte=build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e69503e-f440-4df8-a5d1-a442f7c743f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "g=torch.Generator().manual_seed(2147483647) #reproducability\n",
    "C=torch.randn((27,10),generator=g)\n",
    "W1=torch.randn((30,200),generator=g)\n",
    "b1=torch.randn(200,generator=g)\n",
    "W2=torch.randn((200,27),generator=g)\n",
    "b2=torch.randn(27,generator=g)\n",
    "parameters=[C,W1,b1,W2,b2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64974ca-be72-4b47-8f63-9bf103d43935",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78c1e29-f7ed-4072-a37d-f5c1c4dfca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63af8de4-956e-4a13-893f-5b74d7e255d9",
   "metadata": {},
   "source": [
    "### finding best learnig rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cd0b24-6fb9-4308-a9a7-9f8414b1481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#experiment with to find upper bound and lower bound a,these bound can be idnetified by seeing unstable loss\n",
    "\n",
    "lre=torch.linspace(-3,0,1000)\n",
    "lrs=10**lre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0301d27b-4168-4b70-a0ca-f98db51fe30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lri=[]\n",
    "lossi=[]\n",
    "stepi=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e9decb-5b80-4480-a523-c35aed209936",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(100000):\n",
    "    #minibatch construct since ntire dataset takes lot of computation\n",
    "    ix=torch.randint(0,Xtre.shape[0],(32,))\n",
    "    #forward pass\n",
    "    emb=C[Xtre[ix]] #(32,3,2)\n",
    "    h=torch.tanh(emb.view(-1,30)@W1+b1) #32,100\n",
    "    logits=h@W2+b2 #32,27\n",
    "    # counts=logits.exp()\n",
    "    # probs=counts/counts.sum(1,keepdim=True)\n",
    "    # loss=-probs[torch.arange(32),Y].log().mean()\n",
    "    loss=F.cross_entropy(logits,Ytre[ix]) #forwardpass and backwardpass are efficient\n",
    "    # print(loss.item())\n",
    "\n",
    "    #backward\n",
    "    for p in parameters:\n",
    "        p.grad=None\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    #update\n",
    "    # lr=lrs[i]\n",
    "    lr=0.1\n",
    "    if i<50000:\n",
    "        lr=0.1\n",
    "    else:\n",
    "        lr=0.01\n",
    "    for p in parameters:\n",
    "        p.data+=-lr*p.grad\n",
    "    # lri.append(lr)\n",
    "    stepi.append(i)\n",
    "    lossi.append(loss)\n",
    "    \n",
    "print(loss)\n",
    "#loss not zero ... --> all alphabets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d16cdd-c18b-4200-bcf8-a3f51d5f3978",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(lri),torch.tensor(lossi)) #for determining learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86971042-161b-4b52-9c6a-02dd40c56d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(stepi),torch.tensor(lossi)) #to determine the hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa538d96-8903-4423-a1e5-887ebaab4ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb=C[Xtre] #(32,3,2)\n",
    "h=torch.tanh(emb.view(-1,30)@W1+b1) #32,100\n",
    "logits=h@W2+b2 #32,27\n",
    "loss=F.cross_entropy(logits,Ytre)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390eced8-bbdf-4faa-9ab6-95be574d0dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb=C[Xval] #(32,3,2)\n",
    "h=torch.tanh(emb.view(-1,30)@W1+b1) #32,100\n",
    "logits=h@W2+b2 #32,27\n",
    "loss=F.cross_entropy(logits,Yval)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab978bd-52ed-4abe-ab3f-17e35a0fe086",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding plot for two dimensional embeddings\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "plt.scatter(C[:,0].data,C[:,1].data,s=200)\n",
    "for i in  range(C.shape[0]):\n",
    "    plt.text(C[i,0].item(),C[i,1].item(),itos[i],ha=\"center\",va=\"center\",color=\"white\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9905f4aa-041c-48f4-a5ca-4a04624ec0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#note :\n",
    "#in this by using batches we find approx grad rather than exact grad,it suitable in  practice\n",
    "#as the size of the network increases it may lead s to overfitting loss~0\n",
    "#training split,dev/val split ,test split\n",
    "# If the validation accuracy is much lower than the training accuracy, the model might be overfitting (memorizing instead of generalizing).\n",
    "#80%,10%,10%\n",
    "#we can increase performace we can increase network size or increase dimentions of embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6296062a-2ba0-4b12-b302-4076f4a39266",
   "metadata": {},
   "source": [
    "#### justification for cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d9f61e-4bd5-4014-b8a1-2645ab004dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logits=torch.tensor([-5,-3,0,100])\n",
    "counts=logits.exp()\n",
    "p=counts/counts.sum()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7163f1f9-6265-410c-9be3-d463187dd3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for more positive value logits.exp become inf\n",
    "#for more negative values logits.exp become small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ae4700-d2e2-4322-87a7-b9142f6638a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross entropy\n",
    "logits=logits-logits.max()\n",
    "counts=logits.exp()\n",
    "p=counts/counts.sum()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b00ce4-f3aa-4334-985f-c85558be86d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampling from model\n",
    "for i in range(10):\n",
    "    out=[]\n",
    "    \n",
    "    context=[0]*3\n",
    "    while True:\n",
    "        emb=C[torch.tensor(context)] #(1,blocksize,d)\n",
    "        h=emb.view(1,-1)@W1+b1\n",
    "        logits=h@W2+b2\n",
    "        probs=F.softmax(logits)\n",
    "\n",
    "        ix=torch.multinomial(probs,replacement=True,num_samples=1).item()\n",
    "        context=context[1:]+[ix]\n",
    "        out.append(ix)\n",
    "        if ix==0:\n",
    "            break\n",
    "    print('.'join(itos[i] for i in out))\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5414e9-c7a5-40bd-a29e-086f1399350c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
