{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfab7b5d-9d43-4bbb-b82c-7c7b6a5f9eda",
   "metadata": {},
   "source": [
    "## Image classification using convolutional neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6885add0-67fd-415d-9228-35348eff05f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import random_split,DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tarfile\n",
    "from torchvision.datasets.utils import download_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38473b94-b997-4352-a44c-12c86fccbbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name=\"5_cifar10\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a61735-29be-46a4-b560-8a6e4c00c180",
   "metadata": {},
   "source": [
    "### Download and setup dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c243eeb-9ab7-4dc3-82f7-575a73b919cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_url=\"https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz\"\n",
    "download_url(dataset_url,\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a975bf8-3de6-4646-b194-a21c7edfc328",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tarfile.open(\"./data/cifar10.tgz\",\"r:gz\") as tar:\n",
    "    tar.extractall(path=\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d78de4-6fc0-4568-8def-7b1ca4024301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir(\"./data/cifar10\"))\n",
    "print(os.listdir(\"./data/cifar10/train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4479f5-89af-4379-a04f-7d7f239bfb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(os.listdir(\"./data/cifar10/train/cat\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa965c2c-a82f-4647-a073-c1e8a555be79",
   "metadata": {},
   "source": [
    "this directory structure is used by many datasets,pytorch provide a utility ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ca8030-2e8b-43ea-a77e-ec9e9e8c59c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "dataset=ImageFolder(\"./data/cifar10/train\",transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005189d0-46ca-4427-b6b8-df5745fda186",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e740c9c-c644-4a99-b04f-bd3c826b812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13436d3e-7201-4b32-93d8-f67221d5c0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image,label=dataset[0]\n",
    "print(image.shape)\n",
    "plt.imshow(image.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd95fea-fc49-4b01-9988-133ebc16e093",
   "metadata": {},
   "source": [
    "Training set-Used to train and compute loss and adjust weights of the model   \n",
    "Validation_set-Used to evaluate the model while training,adjust hyperparameters and pick the best version of model  \n",
    "Test set-used to compare different models or different modelling approches adn report final accuracy of the model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac43bc3-46c7-4c1e-86a9-86316caaed4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train and validation spilt\n",
    "\n",
    "train_ds,val_ds=random_split(dataset,[40000,10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305d0f8c-f605-4de4-95f6-a733de679bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataLoaders\n",
    "train_dl=DataLoader(train_ds,batch_size=128,num_workers=4,pin_memory=True,shuffle=True)\n",
    "val_dl=DataLoader(val_ds,batch_size=128,num_workers=4,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1819fd22-3c26-4426-8c0b-88e3f79921e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds=ImageFolder(\"./data/cifar10/test\",transform=transforms.ToTensor())\n",
    "test_dl=DataLoader(test_ds,batch_size=256,num_workers=4,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f32d95-89c7-4789-aa1f-79a4693ab739",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "def show_batch(dl):\n",
    "    for images,labels in dl:\n",
    "        fig,ax=plt.subplots(figsize=(12,6))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images,nrow=16).permute(1,2,0))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bc7313-37a3-4540-97ff-c89a47291b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2456dbff-2d83-4840-a0a9-b0202132de62",
   "metadata": {},
   "source": [
    "### Defining the Model (convolutional neural network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344a3b88-ee2f-4d4f-adba-3e2aa1e04076",
   "metadata": {},
   "source": [
    "<b>Convolution</b>  \n",
    "-> kernel is small matrix of weights.this kernel slides over the 2D input data ,performing an elementwise multiplication with the part of the input it is currently on,and then summing up the results into a single output pixel    \n",
    "-> for multichannel images, a diffent kernel is applied to each channels, and the outputs(feature maps) are added together pixel wise,we get output map  \n",
    "->the output maps are new channels  \n",
    "->no of kernels== no of new channels  \n",
    "->trying to increse the no of channels by using kernels  \n",
    "->as edges covering fewer times than other pixels by kernels we add padding <i>(if padding=1 then input dim=ouput dim)</i> \n",
    " \n",
    "->RESOURCE  \n",
    "1.intuitively understanding convolutions for deep learning by irhum shafkat    \n",
    "2.convolutions in depth by sylvian gugger   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcbc1f7-98c5-40d5-a3dc-e2856008eaf3",
   "metadata": {},
   "source": [
    "<b>Pooling</b>  \n",
    "we want an output with lower size than the input.so,reduce size of spatial dimensions when number of channels increses.one way of accomplish this is by using pooling layer(eg:taking the average/max of every 2X2 grid to reduce each spatial dimensions in half) and another way is stride  \n",
    "-> to move kernel more then one position at a time we add stride "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aba6c5-4679-4fd3-8c8c-749ebdd61aea",
   "metadata": {},
   "source": [
    "Advantages of CNN  \n",
    "-fewer parameters  \n",
    "-sparsity of connections:In each layer,each otp element only depends on small num of inputs elements which makes the forward and backward pass more efficient  \n",
    "-parameter sharing and spatial invariance: the features learned by kernel in one part of the image can be used to detect similar pattern in a differnt  part of another image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dbc998-9607-4873-8460-5aa06603aa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv=nn.Conv2d(3,8,kernel_size=3,stride=1,padding=1) #input channel ,output channel/num_kernels\n",
    "pool=nn.MaxPool2d(2,2)\n",
    "for images,labels in train_dl:\n",
    "    print(images.shape)\n",
    "    out=conv(images)\n",
    "    print(out.shape)\n",
    "    out=pool(out)\n",
    "    print(out.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd2fb2b-96fe-49e3-aa46-a0a29e39baac",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.weight #diff weights for diff channel "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3843ca7-ed08-4b7d-ab6e-9d45570d0be2",
   "metadata": {},
   "source": [
    "the conv2d layer transforms a 3 channel image to n-channel feature map and the maxpool layer halves the height and width.the feature map gets smaller as we add more layers until we are finlly left with a small feature map,which can be flattened into vector.we can then add some fully connected layers at the end to get vector of size 10 for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0befb8c1-3d22-49c1-a78e-ba3290490369",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image classification base\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self,batch):\n",
    "        images,labels=batch\n",
    "        outputs=self(images)\n",
    "        loss=F.cross_entropy(outputs,labels)\n",
    "        return loss\n",
    "    def validation_step(self,batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "    def validation_epoch_end(self,outputs):\n",
    "        acc=[x[\"val_acc\"] for x in outputs]\n",
    "        loss=[x['val_loss'] for x in outputs]\n",
    "\n",
    "        mean_acc=torch.stack(acc).mean()\n",
    "        mean_loss=torch.stack(loss).mean()\n",
    "        return {\"val_loss\":mean_loss.item(),\"val_acc\":mean_acc.item()}\n",
    "    def epoch_end(self,epoch,result):\n",
    "        print(\"Epoch [{}], train_loss:{:.4f} val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result[\"train_loss\"],result['val_loss'], result['val_acc']))\n",
    "\n",
    "\n",
    "def accuracy(outputs,labels):\n",
    "    _,preds=torch.max(outputs,dim=1)\n",
    "    return torch.tensor(torch.sum(preds==labels).item()/len(preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1980e76e-2c11-421e-9300-dbbaecad5261",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10cnnModel(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network=nn.Sequential(\n",
    "            #input 3X32X32\n",
    "            nn.Conv2d(3,32,kernel_size=3,stride=1,padding=1),\n",
    "            #output 32X32X32\n",
    "            nn.ReLU(),\n",
    "            #output 32X32X32\n",
    "            nn.Conv2d(32,64,kernel_size=3,stride=1,padding=1),\n",
    "            #output 64X32X32\n",
    "            nn.ReLU(),\n",
    "            #output 64X32X32\n",
    "            nn.MaxPool2d(2,2),#64X16X16\n",
    "\n",
    "            nn.Conv2d(64,128,kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128,128,kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),#128X8X8\n",
    "\n",
    "            nn.Conv2d(128,256,kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256,256,kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),#256X4X4\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256*4*4,1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,10))\n",
    "    def forward(self,xb):\n",
    "        return self.network(xb)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56efbb8-11cb-4f5f-b178-db7ace540d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Cifar10cnnModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54514f09-c924-4031-b2c5-ef26285a3503",
   "metadata": {},
   "source": [
    "### Using GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca163c7-d688-4d53-a7ed-2b8ea25f151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377fe452-19e8-473a-9dae-73808fe3acf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e213a5-3d44-4c77-a664-7913b6eb8a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(data,device):\n",
    "    if(isinstance(data,(list,tuple))):\n",
    "        return [to_device(x,device) for x in data]\n",
    "    return data.to(device,non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210aeef9-4722-4de0-8cd2-a3556f190851",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Device_Data_Loader():\n",
    "    def __init__(self,dl,device):\n",
    "        self.dl=dl\n",
    "        self.device=device\n",
    "    def __iter__(self):\n",
    "        for x in self.dl:\n",
    "            yield to_device(x,device)\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cd52dd-3fde-497b-9fe6-5bb2cf8133f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl=Device_Data_Loader(train_dl,device)\n",
    "val_dl=Device_Data_Loader(val_dl,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78459469-f33f-4d51-9700-3275975063f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "554901cc-73c0-4084-9ef0-78406772541e",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c81759-ca15-49ad-8738-c7a8e96880a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs,lr,model,train_dl,val_dl,opt_fn=torch.optim.SGD):\n",
    "    opt_fn=opt_fn(model.parameters(),lr)\n",
    "    history=[]\n",
    "    \n",
    "    #training phase\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_losses=[]\n",
    "        for batch in train_dl:\n",
    "            loss=model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            opt_fn.step()\n",
    "            opt_fn.zero_grad()\n",
    "        #validation phase\n",
    "        result=evaluate(model,val_dl)\n",
    "        result[\"train_loss\"]=torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch,result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e6db1c-df37-4e6e-b61a-452350781368",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model,val_dl):\n",
    "    model.eval()\n",
    "    outputs= [model.validation_step(batch) for batch in val_dl]\n",
    "    return model.validation_epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a972ee9e-f70f-45f6-9a94-b436216954ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=to_device(model,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1355ce6c-9dd8-4d35-82a3-5a5224f29355",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist=[evaluate(model,val_dl)]\n",
    "print(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff2b682-17cf-48cc-b5ef-d4c6e91798d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images,labels in train_dl:\n",
    "    print(images.shape)\n",
    "    out=model(images)\n",
    "    print(out.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa9a784-b43d-4917-b2c0-3ed7c8358cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=10\n",
    "opt_func=torch.optim.Adam\n",
    "lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8e36a3-8fc1-4724-958a-a762e05a93ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist=fit(num_epochs,lr,model,train_dl,val_dl,opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7740ba9-d9c9-4a16-9bc1-f9c0348dd8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_Accuracies(history):\n",
    "    plt.plot([x[\"val_acc\"] for x in history])\n",
    "    plt.title(\"acc vs num_epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148f537a-559c-4191-9fbb-fd9ad938559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_Accuracies(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b99f99-e920-44e2-b114-687b285bb445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(history):\n",
    "    train_losses=[x[\"train_loss\"] for x in history]\n",
    "    val_losses=[x[\"val_loss\"] for x in history]\n",
    "    plt.plot(train_losses,\"-bx\")\n",
    "    plt.plot(val_losses,\"-rx\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.legend([\"Training\",\"Validation\"])\n",
    "    plt.title(\"loss vs No of epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976cbbc6-71c9-46ce-9b2a-439b26b4556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf67c82-e353-4f26-af3b-7c133b3ca4e4",
   "metadata": {},
   "source": [
    "here overfitting occur  \n",
    "- gathering and generating more dat  \n",
    "-regularization  \n",
    "-early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6154e504-a225-4770-91fe-27c1c112642a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
